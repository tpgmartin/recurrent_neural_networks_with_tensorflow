{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python.framework import ops\n",
    "# ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters - these come straight from the example code of \"TensorFlow Cookbook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_word_freq = 5 # Trim the less frequent words off\n",
    "rnn_size = 128 # RNN Model size\n",
    "embedding_size = 100 # Word embedding size\n",
    "epochs = 10 # Number of epochs to cycle through data\n",
    "batch_size = 100 # Train on this many examples at once\n",
    "learning_rate = 0.001 # Learning rate\n",
    "training_seq_len = 50 # how long of a word group to consider \n",
    "embedding_size = rnn_size\n",
    "save_every = 500 # How often to save model checkpoints\n",
    "eval_every = 50 # How often to evaluate the test sentences\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']\n",
    "\n",
    "# Download/store Shakespeare data\n",
    "data_dir = 'temp'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "\n",
    "# Declare punctuation to remove, everything except hyphens and apostrophes\n",
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "\n",
    "# Make data directory\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data if not already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare data\n",
      "Cleaning Text\n",
      "Done loading/cleaning.\n"
     ]
    }
   ],
   "source": [
    "print('Loading Shakespeare data')\n",
    "# Check if file is downloaded\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare text from www.gutenbery.org')\n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    # Get Shakespeare text\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    # Decode binary into string\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    # Drop first few descriptive paragraphs\n",
    "    s_text = s_text[7675:]\n",
    "    # Remove newlines\n",
    "    s_text = s_text.replace('\\r\\a', '')\n",
    "    s_text = s_text.replace('\\a', '')\n",
    "    \n",
    "    # Write to file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "        \n",
    "else:\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')\n",
    "\n",
    "# Clean text\n",
    "print('Cleaning Text')\n",
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text).strip().lower()\n",
    "print('Done loading/cleaning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build word processing dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    word_counts = {key:val for key, val in word_counts.items() if val>min_word_freq}\n",
    "    words = word_counts.keys()\n",
    "    # Create vocab to index mapping\n",
    "    vocab_to_ix_dict = {key:(ix+1) for ix, key in enumerate(words)}\n",
    "    # Add unknown key\n",
    "    vocab_to_ix_dict['unknown'] = 0\n",
    "    # Create index to vocab mapping\n",
    "    ix_to_vocab_dict = {val:key for key, val in vocab_to_ix_dict.items()}\n",
    "    \n",
    "    return (ix_to_vocab_dict, vocab_to_ix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buidling Shakespeare Vocab\n",
      "Vocabulary Length = 8009\n"
     ]
    }
   ],
   "source": [
    "print('Buidling Shakespeare Vocab')\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "assert(len(ix2vocab) == len(vocab2ix))\n",
    "vocab_size = len(ix2vocab) + 1\n",
    "print('Vocabulary Length = {}'.format(vocab_size))\n",
    "\n",
    "# Convert words to indexes\n",
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    \n",
    "    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate, \n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # inferring meaning generating text - there is a function that handles this below\n",
    "        # This is used in the testing stage of our network\n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size)\n",
    "        # return zero-filled state tensor\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        # tf.placeholder(dtype, size)\n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            \n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "            \n",
    "            # Define embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.embedding_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        # Add a 'loop' function to generate text if we are inferring\n",
    "        def inferred_loop(prev, count):\n",
    "            # Apply hidden layer\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            # Get the index of the output (don't run the gradient)\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            # Get embedded vector\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return (output)\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed, \n",
    "                                      self.initial_state, \n",
    "                                      self.lstm_cell, \n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        \n",
    "        # Non inferred outputs\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output], [tf.reshape(self.y_output, [-1])], \n",
    "                        [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                        self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "    \n",
    "    # Generate text sample\n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text='thou art'):\n",
    "        # initialise cell\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1,1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "        \n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1,1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return (out_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Share the variable scope between the trained and test model, to use the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate, training_seq_len, vocab_size)\n",
    "\n",
    "# Reuse scope for testing\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate, \n",
    "                                 training_seq_len, vocab_size, infer_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a model saving operation\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create batches for each epoch\n",
    "num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1\n",
    "# Split text indices into subarrays\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "# Reshape each split\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch #1 of 10\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 182, Loss: 9.92\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 182, Loss: 9.20\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 182, Loss: 8.71\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 182, Loss: 8.28\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 182, Loss: 8.17\n",
      "thou art more in the\n",
      "to be or not to the\n",
      "wherefore art thou excels street not to the\n",
      "Iteration: 60, Epoch: 1, Batch: 60 out of 182, Loss: 7.80\n",
      "Iteration: 70, Epoch: 1, Batch: 70 out of 182, Loss: 7.54\n",
      "Iteration: 80, Epoch: 1, Batch: 80 out of 182, Loss: 7.61\n",
      "Iteration: 90, Epoch: 1, Batch: 90 out of 182, Loss: 7.25\n",
      "Iteration: 100, Epoch: 1, Batch: 100 out of 182, Loss: 7.09\n",
      "thou art more\n",
      "to be or not to the\n",
      "wherefore art thou\n",
      "Iteration: 110, Epoch: 1, Batch: 110 out of 182, Loss: 7.10\n",
      "Iteration: 120, Epoch: 1, Batch: 120 out of 182, Loss: 6.92\n",
      "Iteration: 130, Epoch: 1, Batch: 130 out of 182, Loss: 6.82\n",
      "Iteration: 140, Epoch: 1, Batch: 140 out of 182, Loss: 6.77\n",
      "Iteration: 150, Epoch: 1, Batch: 150 out of 182, Loss: 6.80\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 160, Epoch: 1, Batch: 160 out of 182, Loss: 6.91\n",
      "Iteration: 170, Epoch: 1, Batch: 170 out of 182, Loss: 6.70\n",
      "Iteration: 180, Epoch: 1, Batch: 180 out of 182, Loss: 6.62\n",
      "Starting Epoch #2 of 10\n",
      "Iteration: 190, Epoch: 2, Batch: 9 out of 182, Loss: 6.52\n",
      "Iteration: 200, Epoch: 2, Batch: 19 out of 182, Loss: 6.60\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 210, Epoch: 2, Batch: 29 out of 182, Loss: 6.40\n",
      "Iteration: 220, Epoch: 2, Batch: 39 out of 182, Loss: 6.41\n",
      "Iteration: 230, Epoch: 2, Batch: 49 out of 182, Loss: 6.21\n",
      "Iteration: 240, Epoch: 2, Batch: 59 out of 182, Loss: 6.31\n",
      "Iteration: 250, Epoch: 2, Batch: 69 out of 182, Loss: 6.39\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 260, Epoch: 2, Batch: 79 out of 182, Loss: 6.21\n",
      "Iteration: 270, Epoch: 2, Batch: 89 out of 182, Loss: 6.33\n",
      "Iteration: 280, Epoch: 2, Batch: 99 out of 182, Loss: 6.44\n",
      "Iteration: 290, Epoch: 2, Batch: 109 out of 182, Loss: 6.10\n",
      "Iteration: 300, Epoch: 2, Batch: 119 out of 182, Loss: 6.21\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 310, Epoch: 2, Batch: 129 out of 182, Loss: 6.25\n",
      "Iteration: 320, Epoch: 2, Batch: 139 out of 182, Loss: 6.20\n",
      "Iteration: 330, Epoch: 2, Batch: 149 out of 182, Loss: 6.18\n",
      "Iteration: 340, Epoch: 2, Batch: 159 out of 182, Loss: 6.13\n",
      "Iteration: 350, Epoch: 2, Batch: 169 out of 182, Loss: 6.17\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art not in the\n",
      "Iteration: 360, Epoch: 2, Batch: 179 out of 182, Loss: 6.23\n",
      "Starting Epoch #3 of 10\n",
      "Iteration: 370, Epoch: 3, Batch: 8 out of 182, Loss: 6.61\n",
      "Iteration: 380, Epoch: 3, Batch: 18 out of 182, Loss: 6.58\n",
      "Iteration: 390, Epoch: 3, Batch: 28 out of 182, Loss: 6.51\n",
      "Iteration: 400, Epoch: 3, Batch: 38 out of 182, Loss: 6.21\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art not in the\n",
      "Iteration: 410, Epoch: 3, Batch: 48 out of 182, Loss: 6.31\n",
      "Iteration: 420, Epoch: 3, Batch: 58 out of 182, Loss: 6.28\n",
      "Iteration: 430, Epoch: 3, Batch: 68 out of 182, Loss: 6.13\n",
      "Iteration: 440, Epoch: 3, Batch: 78 out of 182, Loss: 5.93\n",
      "Iteration: 450, Epoch: 3, Batch: 88 out of 182, Loss: 5.99\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art not\n",
      "Iteration: 460, Epoch: 3, Batch: 98 out of 182, Loss: 6.10\n",
      "Iteration: 470, Epoch: 3, Batch: 108 out of 182, Loss: 5.96\n",
      "Iteration: 480, Epoch: 3, Batch: 118 out of 182, Loss: 6.19\n",
      "Iteration: 490, Epoch: 3, Batch: 128 out of 182, Loss: 5.95\n",
      "Iteration: 500, Epoch: 3, Batch: 138 out of 182, Loss: 6.21\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 510, Epoch: 3, Batch: 148 out of 182, Loss: 6.10\n",
      "Iteration: 520, Epoch: 3, Batch: 158 out of 182, Loss: 6.33\n",
      "Iteration: 530, Epoch: 3, Batch: 168 out of 182, Loss: 6.06\n",
      "Iteration: 540, Epoch: 3, Batch: 178 out of 182, Loss: 6.16\n",
      "Starting Epoch #4 of 10\n",
      "Iteration: 550, Epoch: 4, Batch: 7 out of 182, Loss: 6.12\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 560, Epoch: 4, Batch: 17 out of 182, Loss: 5.94\n",
      "Iteration: 570, Epoch: 4, Batch: 27 out of 182, Loss: 6.08\n",
      "Iteration: 580, Epoch: 4, Batch: 37 out of 182, Loss: 5.97\n",
      "Iteration: 590, Epoch: 4, Batch: 47 out of 182, Loss: 6.06\n",
      "Iteration: 600, Epoch: 4, Batch: 57 out of 182, Loss: 6.02\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 610, Epoch: 4, Batch: 67 out of 182, Loss: 6.10\n",
      "Iteration: 620, Epoch: 4, Batch: 77 out of 182, Loss: 5.72\n",
      "Iteration: 630, Epoch: 4, Batch: 87 out of 182, Loss: 6.10\n",
      "Iteration: 640, Epoch: 4, Batch: 97 out of 182, Loss: 6.22\n",
      "Iteration: 650, Epoch: 4, Batch: 107 out of 182, Loss: 6.15\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 660, Epoch: 4, Batch: 117 out of 182, Loss: 5.86\n",
      "Iteration: 670, Epoch: 4, Batch: 127 out of 182, Loss: 6.20\n",
      "Iteration: 680, Epoch: 4, Batch: 137 out of 182, Loss: 6.01\n",
      "Iteration: 690, Epoch: 4, Batch: 147 out of 182, Loss: 6.15\n",
      "Iteration: 700, Epoch: 4, Batch: 157 out of 182, Loss: 6.23\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art not\n",
      "Iteration: 710, Epoch: 4, Batch: 167 out of 182, Loss: 6.21\n",
      "Iteration: 720, Epoch: 4, Batch: 177 out of 182, Loss: 6.23\n",
      "Starting Epoch #5 of 10\n",
      "Iteration: 730, Epoch: 5, Batch: 6 out of 182, Loss: 5.81\n",
      "Iteration: 740, Epoch: 5, Batch: 16 out of 182, Loss: 6.04\n",
      "Iteration: 750, Epoch: 5, Batch: 26 out of 182, Loss: 5.94\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art not in the\n",
      "Iteration: 760, Epoch: 5, Batch: 36 out of 182, Loss: 6.12\n",
      "Iteration: 770, Epoch: 5, Batch: 46 out of 182, Loss: 6.06\n",
      "Iteration: 780, Epoch: 5, Batch: 56 out of 182, Loss: 6.05\n",
      "Iteration: 790, Epoch: 5, Batch: 66 out of 182, Loss: 6.01\n",
      "Iteration: 800, Epoch: 5, Batch: 76 out of 182, Loss: 5.82\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art not to the\n",
      "Iteration: 810, Epoch: 5, Batch: 86 out of 182, Loss: 5.86\n",
      "Iteration: 820, Epoch: 5, Batch: 96 out of 182, Loss: 6.03\n",
      "Iteration: 830, Epoch: 5, Batch: 106 out of 182, Loss: 6.00\n",
      "Iteration: 840, Epoch: 5, Batch: 116 out of 182, Loss: 5.95\n",
      "Iteration: 850, Epoch: 5, Batch: 126 out of 182, Loss: 6.03\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 860, Epoch: 5, Batch: 136 out of 182, Loss: 6.11\n",
      "Iteration: 870, Epoch: 5, Batch: 146 out of 182, Loss: 6.15\n",
      "Iteration: 880, Epoch: 5, Batch: 156 out of 182, Loss: 5.91\n",
      "Iteration: 890, Epoch: 5, Batch: 166 out of 182, Loss: 5.89\n",
      "Iteration: 900, Epoch: 5, Batch: 176 out of 182, Loss: 6.15\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art not\n",
      "Starting Epoch #6 of 10\n",
      "Iteration: 910, Epoch: 6, Batch: 5 out of 182, Loss: 5.78\n",
      "Iteration: 920, Epoch: 6, Batch: 15 out of 182, Loss: 5.96\n",
      "Iteration: 930, Epoch: 6, Batch: 25 out of 182, Loss: 6.28\n",
      "Iteration: 940, Epoch: 6, Batch: 35 out of 182, Loss: 5.88\n",
      "Iteration: 950, Epoch: 6, Batch: 45 out of 182, Loss: 6.14\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art a\n",
      "Iteration: 960, Epoch: 6, Batch: 55 out of 182, Loss: 5.68\n",
      "Iteration: 970, Epoch: 6, Batch: 65 out of 182, Loss: 5.90\n",
      "Iteration: 980, Epoch: 6, Batch: 75 out of 182, Loss: 5.63\n",
      "Iteration: 990, Epoch: 6, Batch: 85 out of 182, Loss: 6.20\n",
      "Iteration: 1000, Epoch: 6, Batch: 95 out of 182, Loss: 5.72\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 1010, Epoch: 6, Batch: 105 out of 182, Loss: 5.94\n",
      "Iteration: 1020, Epoch: 6, Batch: 115 out of 182, Loss: 6.15\n",
      "Iteration: 1030, Epoch: 6, Batch: 125 out of 182, Loss: 5.90\n",
      "Iteration: 1040, Epoch: 6, Batch: 135 out of 182, Loss: 5.88\n",
      "Iteration: 1050, Epoch: 6, Batch: 145 out of 182, Loss: 5.91\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 1060, Epoch: 6, Batch: 155 out of 182, Loss: 6.07\n",
      "Iteration: 1070, Epoch: 6, Batch: 165 out of 182, Loss: 5.97\n",
      "Iteration: 1080, Epoch: 6, Batch: 175 out of 182, Loss: 6.07\n",
      "Starting Epoch #7 of 10\n",
      "Iteration: 1090, Epoch: 7, Batch: 4 out of 182, Loss: 5.91\n",
      "Iteration: 1100, Epoch: 7, Batch: 14 out of 182, Loss: 5.95\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art not have been\n",
      "Iteration: 1110, Epoch: 7, Batch: 24 out of 182, Loss: 5.70\n",
      "Iteration: 1120, Epoch: 7, Batch: 34 out of 182, Loss: 6.04\n",
      "Iteration: 1130, Epoch: 7, Batch: 44 out of 182, Loss: 5.73\n",
      "Iteration: 1140, Epoch: 7, Batch: 54 out of 182, Loss: 5.88\n",
      "Iteration: 1150, Epoch: 7, Batch: 64 out of 182, Loss: 5.98\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 1160, Epoch: 7, Batch: 74 out of 182, Loss: 5.82\n",
      "Iteration: 1170, Epoch: 7, Batch: 84 out of 182, Loss: 6.02\n",
      "Iteration: 1180, Epoch: 7, Batch: 94 out of 182, Loss: 5.96\n",
      "Iteration: 1190, Epoch: 7, Batch: 104 out of 182, Loss: 5.89\n",
      "Iteration: 1200, Epoch: 7, Batch: 114 out of 182, Loss: 5.64\n",
      "thou art more than\n",
      "to be or not to\n",
      "wherefore art thou art thou art thou art thou\n",
      "Iteration: 1210, Epoch: 7, Batch: 124 out of 182, Loss: 5.96\n",
      "Iteration: 1220, Epoch: 7, Batch: 134 out of 182, Loss: 6.08\n",
      "Iteration: 1230, Epoch: 7, Batch: 144 out of 182, Loss: 6.03\n",
      "Iteration: 1240, Epoch: 7, Batch: 154 out of 182, Loss: 5.80\n",
      "Iteration: 1250, Epoch: 7, Batch: 164 out of 182, Loss: 6.08\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art not have been\n",
      "Iteration: 1260, Epoch: 7, Batch: 174 out of 182, Loss: 5.75\n",
      "Starting Epoch #8 of 10\n",
      "Iteration: 1270, Epoch: 8, Batch: 3 out of 182, Loss: 5.80\n",
      "Iteration: 1280, Epoch: 8, Batch: 13 out of 182, Loss: 5.96\n",
      "Iteration: 1290, Epoch: 8, Batch: 23 out of 182, Loss: 5.51\n",
      "Iteration: 1300, Epoch: 8, Batch: 33 out of 182, Loss: 5.76\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art not have been\n",
      "Iteration: 1310, Epoch: 8, Batch: 43 out of 182, Loss: 6.03\n",
      "Iteration: 1320, Epoch: 8, Batch: 53 out of 182, Loss: 5.80\n",
      "Iteration: 1330, Epoch: 8, Batch: 63 out of 182, Loss: 5.54\n",
      "Iteration: 1340, Epoch: 8, Batch: 73 out of 182, Loss: 5.83\n",
      "Iteration: 1350, Epoch: 8, Batch: 83 out of 182, Loss: 5.87\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou canst thou art thou art thou art thou art thou\n",
      "Iteration: 1360, Epoch: 8, Batch: 93 out of 182, Loss: 5.58\n",
      "Iteration: 1370, Epoch: 8, Batch: 103 out of 182, Loss: 5.87\n",
      "Iteration: 1380, Epoch: 8, Batch: 113 out of 182, Loss: 5.91\n",
      "Iteration: 1390, Epoch: 8, Batch: 123 out of 182, Loss: 6.01\n",
      "Iteration: 1400, Epoch: 8, Batch: 133 out of 182, Loss: 5.97\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou liest thou art thou art a\n",
      "Iteration: 1410, Epoch: 8, Batch: 143 out of 182, Loss: 5.72\n",
      "Iteration: 1420, Epoch: 8, Batch: 153 out of 182, Loss: 5.96\n",
      "Iteration: 1430, Epoch: 8, Batch: 163 out of 182, Loss: 5.79\n",
      "Iteration: 1440, Epoch: 8, Batch: 173 out of 182, Loss: 5.98\n",
      "Starting Epoch #9 of 10\n",
      "Iteration: 1450, Epoch: 9, Batch: 2 out of 182, Loss: 5.72\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou canst thou not be\n",
      "Iteration: 1460, Epoch: 9, Batch: 12 out of 182, Loss: 5.82\n",
      "Iteration: 1470, Epoch: 9, Batch: 22 out of 182, Loss: 5.46\n",
      "Iteration: 1480, Epoch: 9, Batch: 32 out of 182, Loss: 5.74\n",
      "Iteration: 1490, Epoch: 9, Batch: 42 out of 182, Loss: 5.60\n",
      "Iteration: 1500, Epoch: 9, Batch: 52 out of 182, Loss: 5.78\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than\n",
      "to be or not to be\n",
      "wherefore art thou canst thou art thou art thou art not to be\n",
      "Iteration: 1510, Epoch: 9, Batch: 62 out of 182, Loss: 5.46\n",
      "Iteration: 1520, Epoch: 9, Batch: 72 out of 182, Loss: 5.70\n",
      "Iteration: 1530, Epoch: 9, Batch: 82 out of 182, Loss: 5.77\n",
      "Iteration: 1540, Epoch: 9, Batch: 92 out of 182, Loss: 5.71\n",
      "Iteration: 1550, Epoch: 9, Batch: 102 out of 182, Loss: 5.52\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou liest thou art thou art thou art thou art thou\n",
      "Iteration: 1560, Epoch: 9, Batch: 112 out of 182, Loss: 5.82\n",
      "Iteration: 1570, Epoch: 9, Batch: 122 out of 182, Loss: 5.85\n",
      "Iteration: 1580, Epoch: 9, Batch: 132 out of 182, Loss: 5.87\n",
      "Iteration: 1590, Epoch: 9, Batch: 142 out of 182, Loss: 6.01\n",
      "Iteration: 1600, Epoch: 9, Batch: 152 out of 182, Loss: 5.70\n",
      "thou art more than the\n",
      "to be or not to\n",
      "wherefore art thou canst thou not have been\n",
      "Iteration: 1610, Epoch: 9, Batch: 162 out of 182, Loss: 5.81\n",
      "Iteration: 1620, Epoch: 9, Batch: 172 out of 182, Loss: 5.82\n",
      "Starting Epoch #10 of 10\n",
      "Iteration: 1630, Epoch: 10, Batch: 1 out of 182, Loss: 5.77\n",
      "Iteration: 1640, Epoch: 10, Batch: 11 out of 182, Loss: 5.74\n",
      "Iteration: 1650, Epoch: 10, Batch: 21 out of 182, Loss: 5.74\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou liest thou art thou art a\n",
      "Iteration: 1660, Epoch: 10, Batch: 31 out of 182, Loss: 5.82\n",
      "Iteration: 1670, Epoch: 10, Batch: 41 out of 182, Loss: 5.60\n",
      "Iteration: 1680, Epoch: 10, Batch: 51 out of 182, Loss: 5.42\n",
      "Iteration: 1690, Epoch: 10, Batch: 61 out of 182, Loss: 5.96\n",
      "Iteration: 1700, Epoch: 10, Batch: 71 out of 182, Loss: 5.68\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou liest thou art not to the\n",
      "Iteration: 1710, Epoch: 10, Batch: 81 out of 182, Loss: 5.70\n",
      "Iteration: 1720, Epoch: 10, Batch: 91 out of 182, Loss: 5.68\n",
      "Iteration: 1730, Epoch: 10, Batch: 101 out of 182, Loss: 5.83\n",
      "Iteration: 1740, Epoch: 10, Batch: 111 out of 182, Loss: 5.71\n",
      "Iteration: 1750, Epoch: 10, Batch: 121 out of 182, Loss: 5.62\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou canst thou not be\n",
      "Iteration: 1760, Epoch: 10, Batch: 131 out of 182, Loss: 5.68\n",
      "Iteration: 1770, Epoch: 10, Batch: 141 out of 182, Loss: 5.66\n",
      "Iteration: 1780, Epoch: 10, Batch: 151 out of 182, Loss: 5.78\n",
      "Iteration: 1790, Epoch: 10, Batch: 161 out of 182, Loss: 5.80\n",
      "Iteration: 1800, Epoch: 10, Batch: 171 out of 182, Loss: 5.72\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art not to the\n",
      "Iteration: 1810, Epoch: 10, Batch: 181 out of 182, Loss: 5.59\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_loss = []\n",
    "iteration_count = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle word indices\n",
    "    random.shuffle(batches)\n",
    "    # Create targets from shuffled batches\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    # Run through current epoch\n",
    "    print('Starting Epoch #{} of {}'.format(epoch+1, epochs))\n",
    "    # Reset initial LSTM state after every epoch\n",
    "    # Set by constructor self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        \n",
    "        # train_op defined in LSTM constructor\n",
    "        # optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        # self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op], feed_dict=training_dict)\n",
    "        \n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "            \n",
    "        # Save the model and the vocab\n",
    "        if iteration_count % save_every == 0:\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step = iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "        \n",
    "        iteration_count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY0yas0RgjDFp7v8DF4EKuxngDXcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e325588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss over time\n",
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
